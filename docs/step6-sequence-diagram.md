# Step 6: Idempotency & Exactly-Once Semantics — Diagrams

## Vấn đề: Duplicate Message Delivery

Kafka guarantees **at-least-once** delivery. Consumer có thể nhận cùng 1 message nhiều lần khi:
- Consumer rebalance (new consumer join/leave group)
- Consumer crash trước khi commit offset
- Network issue giữa consumer và broker
- Retry từ error handler (Step 5)

**Hậu quả nếu không có idempotency:**
- Inventory Service: reserve stock 2 lần → stock bị trừ gấp đôi
- Payment Service: charge customer 2 lần → mất tiền oan
- Order Service: update status nhiều lần (có thể override trạng thái mới hơn)

## Idempotent Consumer Pattern

```mermaid
sequenceDiagram
    participant Kafka as Kafka Topic
    participant Consumer as Consumer (@KafkaListener)
    participant Service as Service Layer
    participant DB as Database
    participant PE as processed_events Table

    Note over Kafka,PE: === FIRST TIME: Message processed successfully ===

    Kafka->>Consumer: Deliver message (eventId=abc-123)
    Consumer->>Service: delegate processing
    Service->>PE: SELECT EXISTS(eventId='abc-123')
    PE-->>Service: false (not processed yet)

    rect rgb(220, 245, 220)
        Note over Service,DB: @Transactional — atomic
        Service->>DB: Execute business logic (reserve stock, save payment, etc.)
        Service->>PE: INSERT (eventId='abc-123', topic, processedAt)
        Note over DB,PE: Both committed in same transaction
    end

    Service-->>Consumer: Processing complete
    Consumer->>Kafka: Commit offset

    Note over Kafka,PE: === DUPLICATE: Same message delivered again ===

    Kafka->>Consumer: Deliver message (eventId=abc-123) — duplicate!
    Consumer->>Service: delegate processing
    Service->>PE: SELECT EXISTS(eventId='abc-123')
    PE-->>Service: true (already processed!)

    rect rgb(255, 245, 220)
        Note over Service: SKIP — log warning and return
        Service-->>Consumer: Already processed, skipping
    end

    Consumer->>Kafka: Commit offset
```

## Idempotent Consumer — Decision Flow

```mermaid
flowchart TD
    A[Message arrives at consumer] --> B[Delegate to Service layer]
    B --> C{eventId exists in<br/>processed_events table?}
    C -->|Yes — Duplicate| D[Log WARN: duplicate detected]
    D --> E[Skip processing — return immediately]
    E --> F[Commit offset ✓]
    C -->|No — First time| G["@Transactional BEGIN"]
    G --> H[Execute business logic]
    H --> I[Save ProcessedEvent record]
    I --> J{Transaction commit success?}
    J -->|Yes| F
    J -->|No — Exception| K[Transaction ROLLBACK]
    K --> L[Both business data AND ProcessedEvent rolled back]
    L --> M[Error handler retries or sends to DLT]

    style D fill:#fff3cd
    style E fill:#fff3cd
    style H fill:#d4edda
    style I fill:#d4edda
```

## ProcessedEvent Table Schema

```
┌─────────────────────────────────────────────────────┐
│                  processed_events                     │
├──────────────┬──────────┬────────────────────────────┤
│ Column       │ Type     │ Description                │
├──────────────┼──────────┼────────────────────────────┤
│ event_id     │ UUID     │ PK — from OrderEvent.eventId│
│ topic        │ VARCHAR  │ Source Kafka topic          │
│ processed_at │ TIMESTAMP│ When event was processed   │
└──────────────┴──────────┴────────────────────────────┘

Exists in: order_db, inventory_db, payment_db
(notification-service uses in-memory Set — stateless)
```

## WHY eventId as Primary Key?

```
OrderEvent {
    eventId: UUID ← unique per event, generated by factory method
    orderId: UUID ← same orderId appears in multiple events
    ...
}

Scenario: Order #42 goes through the saga
├── order.placed    → eventId=aaa  orderId=42
├── order.validated → eventId=bbb  orderId=42
├── order.paid      → eventId=ccc  orderId=42
└── order.completed → eventId=ddd  orderId=42

→ Mỗi event có eventId riêng, dù cùng orderId.
→ Dùng eventId (not orderId) để detect exact duplicate delivery.
```

## End-to-End Flow với Idempotency

```mermaid
sequenceDiagram
    participant Client
    participant OrderSvc as Order Service
    participant Kafka as Kafka
    participant InvSvc as Inventory Service
    participant InvDB as inventory_db
    participant PaySvc as Payment Service
    participant PayDB as payment_db

    Client->>OrderSvc: POST /api/orders
    OrderSvc->>Kafka: publish order.placed (eventId=E1)

    Note over Kafka,InvSvc: Inventory consumes with idempotency check

    Kafka->>InvSvc: order.placed (eventId=E1)
    InvSvc->>InvDB: EXISTS eventId=E1? → NO
    InvSvc->>InvDB: BEGIN TX: reserve stock + save ProcessedEvent(E1)
    InvSvc->>Kafka: publish order.validated (eventId=E2)

    Note over Kafka,InvSvc: ⚠️ Rebalance! Message delivered again

    Kafka->>InvSvc: order.placed (eventId=E1) — DUPLICATE
    InvSvc->>InvDB: EXISTS eventId=E1? → YES
    InvSvc-->>InvSvc: SKIP (stock NOT reserved again ✓)

    Note over Kafka,PaySvc: Payment consumes with idempotency check

    Kafka->>PaySvc: order.validated (eventId=E2)
    PaySvc->>PayDB: EXISTS eventId=E2? → NO
    PaySvc->>PayDB: BEGIN TX: save Payment + save ProcessedEvent(E2)
    PaySvc->>Kafka: publish order.paid (eventId=E3)

    Note over Kafka,OrderSvc: Order Service consumes with idempotency check

    Kafka->>OrderSvc: order.paid (eventId=E3)
    OrderSvc->>OrderSvc: EXISTS eventId=E3? → NO
    OrderSvc->>OrderSvc: Update order → COMPLETED
```

## Notification Service — In-Memory Deduplication

```mermaid
sequenceDiagram
    participant Kafka
    participant NotifSvc as Notification Service
    participant MemSet as ConcurrentHashSet<UUID>

    Note over NotifSvc,MemSet: Stateless service — no database<br/>Uses in-memory Set for best-effort dedup

    Kafka->>NotifSvc: order.completed (eventId=E4)
    NotifSvc->>MemSet: processedEventIds.add(E4)
    MemSet-->>NotifSvc: true (added — first time)
    NotifSvc->>NotifSvc: Send notification ✉️

    Kafka->>NotifSvc: order.completed (eventId=E4) — DUPLICATE
    NotifSvc->>MemSet: processedEventIds.add(E4)
    MemSet-->>NotifSvc: false (already exists!)
    NotifSvc-->>NotifSvc: SKIP — duplicate notification prevented

    Note over MemSet: ⚠️ Lost on restart — best-effort only<br/>Acceptable: duplicate email < lost email
```

## Exactly-Once Semantics (EOS) — Kafka Producer

```
Kafka Producer đã được config từ Step 2:

  spring.kafka.producer:
    acks: all                      ← Wait all replicas acknowledge
    properties:
      enable.idempotence: true     ← Broker deduplicates by producer PID + sequence number

┌─────────────────────────────────────────────────────────────┐
│                   Producer Idempotence                        │
│                                                               │
│  Producer ──send(msg, seq=1)──► Broker  (seq=1 saved ✓)     │
│  Producer ──send(msg, seq=1)──► Broker  (seq=1 duplicate!)  │
│                                          └─ Broker discards  │
│                                                               │
│  → Prevents duplicate messages FROM producer side            │
│  → Already enabled since Step 2 (enable.idempotence=true)   │
└─────────────────────────────────────────────────────────────┘
```

## Idempotency Coverage Summary

| Service              | Idempotency Method            | Protected Operations                        |
|----------------------|-------------------------------|---------------------------------------------|
| Order Service        | `processed_events` table (DB) | completeOrder, failOrder, handlePaymentFailure |
| Inventory Service    | `processed_events` table (DB) | processOrderPlaced, compensateReservation    |
| Payment Service      | `processed_events` table (DB) | processOrderValidated                        |
| Notification Service | In-memory `Set<UUID>`         | All notification methods (best-effort)       |
| Kafka Producer       | `enable.idempotence=true`     | Prevents duplicate publish from producer     |

## Key Concepts Learned

| Concept                      | Description                                              |
|------------------------------|----------------------------------------------------------|
| At-Least-Once Delivery       | Kafka default — consumer may receive same message 2+ times |
| Idempotent Consumer          | Processing same message multiple times = same result      |
| ProcessedEvent Table         | DB-based deduplication — eventId as primary key           |
| Atomic Deduplication         | Check + process + save in same @Transactional             |
| Producer Idempotence         | Broker deduplicates by PID + sequence number              |
| Best-Effort Dedup            | In-memory set for stateless services (lost on restart)    |
